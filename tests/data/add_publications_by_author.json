[
  {
    "filename": "2020-01-01-2011.07960.md",
    "content": "---\ntitle: Explicitly Modeling Syntax in Language Models with Incremental Parsing and\n  a Dynamic Oracle\nvenue: NAACL 2020\nnames: Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron Courville\ntags:\n- NAACL\nlink: https://arxiv.org/abs/2011.07960\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nSyntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models."
  },
  {
    "filename": "2020-01-01-2004.09456.md",
    "content": "---\ntitle: 'StereoSet: Measuring stereotypical bias in pretrained language models'\nvenue: ACL 2020\nnames: Moin Nadeem, Anna Bethke, Siva Reddy\ntags:\n- ACL\nlink: https://arxiv.org/abs/2004.09456\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nA stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu."
  },
  {
    "filename": "2020-01-01-2020.findings-emnlp.221.md",
    "content": "---\ntitle: 'You could have said that instead: Improving Chatbots with Natural Language\n  Feedback'\nvenue: FINDINGS 2020\nnames: Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy\ntags:\n- FINDINGS\nlink: https://www.semanticscholar.org/paper/00c2ba51a53da5c340c3217eabab935a67abafa0\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nThe ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator\u2019s goal is to convert the feedback into a response that answers the user\u2019s previous utterance and to fool the discriminator which distinguishes feedback from natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94%to 75.96% in ranking correct responses on the PERSONACHATdataset, a large improvement given that the original model is already trained on 131k samples."
  },
  {
    "filename": "2020-01-01-2012.13978.md",
    "content": "---\ntitle: 'MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding\n  Pretraining'\nvenue: CLINICALNLP 2020\nnames: Zhi Wen, Xing Han Lu, Siva Reddy\ntags:\n- CLINICALNLP\nlink: https://arxiv.org/abs/2012.13978\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nOne of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks."
  },
  {
    "filename": "2020-01-01-2005.01655.md",
    "content": "---\ntitle: 'Words Aren\u2019t Enough, Their Order Matters: On the Robustness of Grounding Visual\n  Referring Expressions'\nvenue: ACL 2020\nnames: Arjun Reddy Akula, Spandana Gella, Y. Al-Onaizan, Song-Chun Zhu, Siva Reddy\ntags:\n- ACL\nlink: https://arxiv.org/abs/2005.01655\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nVisual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn\u2019t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn\u2019t. Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes. Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task. We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task. Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv."
  },
  {
    "filename": "2020-01-01-259cf65eeae13861031f44cf906d43b155192b10.md",
    "content": "---\ntitle: Explicitly Modeling Syntax in Language Model improves Generalization\nvenue: ArXiv 2020\nnames: Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron C. Courville\ntags:\n- ArXiv\nlink: https://www.semanticscholar.org/paper/259cf65eeae13861031f44cf906d43b155192b10\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nSyntax is fundamental to our thinking about language. Although neural networks are very successful in many tasks, they do not explicitly model syntactic structure. Failing to capture the structure of inputs could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with a one-step look-ahead parser and maintains the conditional probability setting of the standard language model. Experiments show that SOM can achieve strong results in language modeling and syntactic generalization tests, while using fewer parameters then other models."
  },
  {
    "filename": "2020-01-01-2010.07261.md",
    "content": "---\ntitle: Learning Improvised Chatbots from Adversarial Modifications of Natural Language\n  Feedback\nvenue: 'Findings of the Association for Computational Linguistics: EMNLP 2020 2020'\nnames: Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy\ntags:\n- 'Findings of the Association for Computational Linguistics: EMNLP 2020'\nlink: https://arxiv.org/abs/2010.07261\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nThe ubiquitous nature of chatbots and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator's goal is to convert the feedback into a response that answers the user's previous utterance and to fool the discriminator which distinguishes feedback from natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94% to 75.96% in ranking correct responses on the Personachat dataset, a large improvement given that the original model is already trained on 131k samples."
  },
  {
    "filename": "2020-01-01-2009.14786.md",
    "content": "---\ntitle: Measuring Systematic Generalization in Neural Proof Generation with Transformers\nvenue: NeurIPS 2020\nnames: Nicolas Gontier, Koustuv Sinha, Siva Reddy, C. Pal\ntags:\n- NeurIPS\nlink: https://arxiv.org/abs/2009.14786\nauthor: Siva Reddy\ncategories: Publications\n\n---\n\n*{{ page.names }}*\n\n**{{ page.venue }}**\n\n{% include display-publication-links.html pub=page%}\n\n## Abstract\n\nWe are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies."
  }
]